{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a404b77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "PySpark is working!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"PySpark is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6deaeeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>Lincoln Letters</td>\n",
       "      <td>LINCOLN LETTERS By Abraham Lincoln Published b...</td>\n",
       "      <td>letters</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>Lincoln's First Inaugural Address</td>\n",
       "      <td>Lincoln's First Inaugural Address March 4, 186...</td>\n",
       "      <td>letters</td>\n",
       "      <td>3626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>Lincoln's Gettysburg Address, given November 1...</td>\n",
       "      <td>Lincoln's Gettysburg Address, given November 1...</td>\n",
       "      <td>letters</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>Lincoln's Inaugurals, Addresses and Letters (S...</td>\n",
       "      <td>Longman's English Classics LINCOLN'S INAUGURAL...</td>\n",
       "      <td>letters</td>\n",
       "      <td>43649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>Lincoln's Second Inaugural Address</td>\n",
       "      <td>Lincoln's Second Inaugural Address March 4, 18...</td>\n",
       "      <td>letters</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                              title  \\\n",
       "0  Abraham Lincoln                                    Lincoln Letters   \n",
       "1  Abraham Lincoln                  Lincoln's First Inaugural Address   \n",
       "2  Abraham Lincoln  Lincoln's Gettysburg Address, given November 1...   \n",
       "3  Abraham Lincoln  Lincoln's Inaugurals, Addresses and Letters (S...   \n",
       "4  Abraham Lincoln                 Lincoln's Second Inaugural Address   \n",
       "\n",
       "                                                text text_type  word_count  \n",
       "0  LINCOLN LETTERS By Abraham Lincoln Published b...   letters        1065  \n",
       "1  Lincoln's First Inaugural Address March 4, 186...   letters        3626  \n",
       "2  Lincoln's Gettysburg Address, given November 1...   letters         299  \n",
       "3  Longman's English Classics LINCOLN'S INAUGURAL...   letters       43649  \n",
       "4  Lincoln's Second Inaugural Address March 4, 18...   letters         703  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preview = pd.read_csv(\"author_identification_dataset_final.csv\")\n",
    "\n",
    "preview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4401bc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks before balance: 1526225\n",
      "After capping per author: 770655\n",
      "split\n",
      "train    545285\n",
      "dev      113592\n",
      "test     111778\n",
      "Name: count, dtype: int64\n",
      "Saved train to author_chunks_dataset/train.parquet\n",
      "Saved dev to author_chunks_dataset/dev.parquet\n",
      "Saved test to author_chunks_dataset/test.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read CSV (pandas)\n",
    "# -------------------------\n",
    "df = pd.read_csv(\"author_identification_dataset_final.csv\")\n",
    "\n",
    "# Use 'title' as book_id to avoid leakage\n",
    "df = df[['author', 'title', 'text']].rename(columns={'title': 'book_id'})\n",
    "\n",
    "# -------------------------\n",
    "# 2. Parameters\n",
    "# -------------------------\n",
    "MIN_LEN = 1000\n",
    "CHUNK_SIZE = 2000\n",
    "MAX_CHUNKS_PER_AUTHOR = 1000\n",
    "\n",
    "# -------------------------\n",
    "# 3. Chunking function\n",
    "# -------------------------\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, min_len=MIN_LEN):\n",
    "    text = str(text)\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return [c for c in chunks if len(c) >= min_len]\n",
    "\n",
    "# -------------------------\n",
    "# 4. Apply chunking across dataframe\n",
    "# -------------------------\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    chunks = chunk_text(row['text'])\n",
    "    for chunk in chunks:\n",
    "        rows.append({\n",
    "            'author': row['author'],\n",
    "            'book_id': row['book_id'],\n",
    "            'chunk': chunk\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "print(\"Total chunks before balance:\", len(df_chunks))\n",
    "\n",
    "# -------------------------\n",
    "# 5. Cap per author to avoid imbalance\n",
    "# -------------------------\n",
    "df_chunks['rn'] = df_chunks.groupby('author').cumcount() + 1\n",
    "df_balanced = df_chunks[df_chunks['rn'] <= MAX_CHUNKS_PER_AUTHOR].drop(columns=['rn'])\n",
    "print(\"After capping per author:\", len(df_balanced))\n",
    "\n",
    "# -------------------------\n",
    "# 6. Train/Dev/Test split by book_id (no leakage between splits)\n",
    "# -------------------------\n",
    "unique_books = df_balanced['book_id'].unique()\n",
    "train_books, test_books = train_test_split(unique_books, test_size=0.3, random_state=42)\n",
    "dev_books, test_books = train_test_split(test_books, test_size=0.5, random_state=42)\n",
    "\n",
    "def assign_split(book_id):\n",
    "    if book_id in train_books: return 'train'\n",
    "    if book_id in dev_books: return 'dev'\n",
    "    return 'test'\n",
    "\n",
    "df_balanced['split'] = df_balanced['book_id'].map(assign_split)\n",
    "print(df_balanced['split'].value_counts())\n",
    "\n",
    "# -------------------------\n",
    "# 7. Save as Parquet (or CSV)\n",
    "# -------------------------\n",
    "os.makedirs(\"author_chunks_dataset\", exist_ok=True)\n",
    "\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    out = f\"author_chunks_dataset/{split}.parquet\"\n",
    "    df_balanced[df_balanced['split'] == split].to_parquet(out, index=False)\n",
    "    print(f\"Saved {split} to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3e1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks before balance (raw dataset): 1526225\n",
      "Saved full raw dataset (1.5M chunks)\n",
      "After capping per author (balanced dataset): 770655\n",
      "Balanced dataset split counts:\n",
      " split\n",
      "train    545285\n",
      "dev      113592\n",
      "test     111778\n",
      "Name: count, dtype: int64\n",
      "Saved train: 545285 rows to author_chunks_dataset/train.parquet and author_chunks_dataset/train.csv\n",
      "Saved dev: 113592 rows to author_chunks_dataset/dev.parquet and author_chunks_dataset/dev.csv\n",
      "Saved test: 111778 rows to author_chunks_dataset/test.parquet and author_chunks_dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read CSV (pandas)\n",
    "# -------------------------\n",
    "df = pd.read_csv(\"author_identification_dataset_final.csv\")\n",
    "\n",
    "# Use 'title' as book_id to avoid leakage\n",
    "df = df[['author', 'title', 'text']].rename(columns={'title': 'book_id'})\n",
    "\n",
    "# -------------------------\n",
    "# 2. Parameters\n",
    "# -------------------------\n",
    "MIN_LEN = 1000\n",
    "CHUNK_SIZE = 2000\n",
    "MAX_CHUNKS_PER_AUTHOR = 1000\n",
    "\n",
    "# -------------------------\n",
    "# 3. Chunking function\n",
    "# -------------------------\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, min_len=MIN_LEN):\n",
    "    text = str(text)\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return [c for c in chunks if len(c) >= min_len]\n",
    "\n",
    "# -------------------------\n",
    "# 4. Apply chunking across dataframe\n",
    "# -------------------------\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    chunks = chunk_text(row['text'])\n",
    "    for chunk in chunks:\n",
    "        rows.append({\n",
    "            'author': row['author'],\n",
    "            'book_id': row['book_id'],\n",
    "            'chunk': chunk\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "print(\"Total chunks before balance (raw dataset):\", len(df_chunks))\n",
    "\n",
    "# -------------------------\n",
    "# Save the full raw 1.5M dataset\n",
    "# -------------------------\n",
    "os.makedirs(\"author_chunks_dataset\", exist_ok=True)\n",
    "df_chunks.to_parquet(\"author_chunks_dataset/full_dataset.parquet\", index=False)\n",
    "df_chunks.to_csv(\"author_chunks_dataset/full_dataset.csv\", index=False)\n",
    "print(\"Saved full raw dataset (1.5M chunks)\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. Cap per author to avoid imbalance\n",
    "# -------------------------\n",
    "df_chunks['rn'] = df_chunks.groupby('author').cumcount() + 1\n",
    "df_balanced = df_chunks[df_chunks['rn'] <= MAX_CHUNKS_PER_AUTHOR].drop(columns=['rn'])\n",
    "print(\"After capping per author (balanced dataset):\", len(df_balanced))\n",
    "\n",
    "# -------------------------\n",
    "# 6. Train/Dev/Test split by book_id (no leakage)\n",
    "# -------------------------\n",
    "unique_books = df_balanced['book_id'].unique()\n",
    "train_books, test_books = train_test_split(unique_books, test_size=0.3, random_state=42)\n",
    "dev_books, test_books = train_test_split(test_books, test_size=0.5, random_state=42)\n",
    "\n",
    "def assign_split(book_id):\n",
    "    if book_id in train_books: return 'train'\n",
    "    if book_id in dev_books: return 'dev'\n",
    "    return 'test'\n",
    "\n",
    "df_balanced['split'] = df_balanced['book_id'].map(assign_split)\n",
    "print(\"Balanced dataset split counts:\\n\", df_balanced['split'].value_counts())\n",
    "\n",
    "# -------------------------\n",
    "# 7. Save splits as Parquet AND CSV\n",
    "# -------------------------\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    split_df = df_balanced[df_balanced['split'] == split]\n",
    "\n",
    "    # Parquet\n",
    "    out_parquet = f\"author_chunks_dataset/{split}.parquet\"\n",
    "    split_df.to_parquet(out_parquet, index=False)\n",
    "\n",
    "    # CSV\n",
    "    out_csv = f\"author_chunks_dataset/{split}.csv\"\n",
    "    split_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    print(f\"Saved {split}: {len(split_df)} rows to {out_parquet} and {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97685cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
